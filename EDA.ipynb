{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loubenskybelile/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/loubenskybelile/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "import random\n",
    "import keras_cv\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "tf.keras.utils.set_random_seed(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### viewing directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waste-images',\n",
       " 'waste-images/disposable_plastic_cutlery',\n",
       " 'waste-images/food_waste',\n",
       " 'waste-images/test',\n",
       " 'waste-images/test/Food Waste',\n",
       " 'waste-images/test/Trash',\n",
       " 'waste-images/test/Mixed Recycle',\n",
       " 'waste-images/office_paper',\n",
       " 'waste-images/glass_food_jars',\n",
       " 'waste-images/aluminum_soda_cans',\n",
       " 'waste-images/magazines',\n",
       " 'waste-images/clothing',\n",
       " 'waste-images/plastic_shopping_bags',\n",
       " 'waste-images/plastic_soda_bottles',\n",
       " 'waste-images/styrofoam_food_containers',\n",
       " 'waste-images/aerosol_cans',\n",
       " 'waste-images/aluminum_food_cans',\n",
       " 'waste-images/newspaper',\n",
       " 'waste-images/eggshells',\n",
       " 'waste-images/glass_cosmetic_containers',\n",
       " 'waste-images/paper_cups',\n",
       " 'waste-images/plastic_water_bottles',\n",
       " 'waste-images/coffee_grounds',\n",
       " 'waste-images/steel_food_cans',\n",
       " 'waste-images/plastic_cup_lids',\n",
       " 'waste-images/cardboard_packaging',\n",
       " 'waste-images/cardboard_boxes',\n",
       " 'waste-images/plastic_straws',\n",
       " 'waste-images/train',\n",
       " 'waste-images/train/Food Waste',\n",
       " 'waste-images/train/Trash',\n",
       " 'waste-images/train/Mixed Recycle',\n",
       " 'waste-images/styrofoam_cups',\n",
       " 'waste-images/glass_beverage_bottles',\n",
       " 'waste-images/shoes',\n",
       " 'waste-images/plastic_trash_bags',\n",
       " 'waste-images/tea_bags',\n",
       " 'waste-images/plastic_food_containers',\n",
       " 'waste-images/val',\n",
       " 'waste-images/val/Food Waste',\n",
       " 'waste-images/val/Trash',\n",
       " 'waste-images/val/Mixed Recycle',\n",
       " 'waste-images/plastic_detergent_bottles']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waste_type_dir = []\n",
    "\n",
    "for (root, dirs, files) in os.walk('waste-images'):\n",
    "    waste_type_dir.append(root)\n",
    "\n",
    "waste_type_dir\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_type_dir = waste_type_dir[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waste-images/disposable_plastic_cutlery', 'waste-images/food_waste', 'waste-images/test', 'waste-images/test/Food Waste', 'waste-images/test/Trash', 'waste-images/test/Mixed Recycle', 'waste-images/office_paper', 'waste-images/glass_food_jars', 'waste-images/aluminum_soda_cans', 'waste-images/magazines', 'waste-images/clothing', 'waste-images/plastic_shopping_bags', 'waste-images/plastic_soda_bottles', 'waste-images/styrofoam_food_containers', 'waste-images/aerosol_cans', 'waste-images/aluminum_food_cans', 'waste-images/newspaper', 'waste-images/eggshells', 'waste-images/glass_cosmetic_containers', 'waste-images/paper_cups', 'waste-images/plastic_water_bottles', 'waste-images/coffee_grounds', 'waste-images/steel_food_cans', 'waste-images/plastic_cup_lids', 'waste-images/cardboard_packaging', 'waste-images/cardboard_boxes', 'waste-images/plastic_straws', 'waste-images/train', 'waste-images/train/Food Waste', 'waste-images/train/Trash', 'waste-images/train/Mixed Recycle', 'waste-images/styrofoam_cups', 'waste-images/glass_beverage_bottles', 'waste-images/shoes', 'waste-images/plastic_trash_bags', 'waste-images/tea_bags', 'waste-images/plastic_food_containers', 'waste-images/val', 'waste-images/val/Food Waste', 'waste-images/val/Trash', 'waste-images/val/Mixed Recycle', 'waste-images/plastic_detergent_bottles']\n"
     ]
    }
   ],
   "source": [
    "print(waste_type_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # restructuring directory\n",
    "# # Define the working directory (where images are stored)\n",
    "# working_directory = \"waste-images\\\\images\\\\images\"\n",
    "\n",
    "# # Define the base directory where categories will be moved to\n",
    "# base_directory = \"waste-images\"\n",
    "\n",
    "# # Iterate through each category folder under the 'images/images' directory\n",
    "# for category in os.listdir(working_directory):\n",
    "#     category_path = os.path.join(working_directory, category)\n",
    "    \n",
    "#     # Check if it's a directory (should be the category folder)\n",
    "#     if os.path.isdir(category_path):\n",
    "        \n",
    "#         # Create the category folder in the base directory if it doesn't exist\n",
    "#         new_category_path = os.path.join(base_directory, category)\n",
    "#         os.makedirs(new_category_path, exist_ok=True)\n",
    "\n",
    "#         # Iterate through 'default' and 'real_world' subdirectories inside each category\n",
    "#         for subfolder in ['default', 'real_world']:\n",
    "#             subfolder_path = os.path.join(category_path, subfolder)\n",
    "            \n",
    "#             # Check if the subfolder exists\n",
    "#             if os.path.exists(subfolder_path):\n",
    "#                 # Iterate through all image files in the subfolder\n",
    "#                 for img_file in os.listdir(subfolder_path):\n",
    "#                     img_path = os.path.join(subfolder_path, img_file)\n",
    "                    \n",
    "#                     # Define the new file path in the base category directory\n",
    "#                     new_img_path = os.path.join(new_category_path, f\"{subfolder}_{img_file}\")\n",
    "                    \n",
    "#                     # Copy the file to the new category directory\n",
    "#                     shutil.copy(img_path, new_img_path)\n",
    "                    \n",
    "#                     # Optionally, you can delete the original file after copying\n",
    "#                     os.remove(img_path)\n",
    "                \n",
    "#                 # After copying, you can remove the empty subdirectory (if you want to clean up)\n",
    "#                 os.rmdir(subfolder_path)\n",
    "\n",
    "# print(\"Images successfully moved and subdirectories cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to count images per subfolder\n",
    "# def count_images_per_folder(waste_type_dir):\n",
    "#     for category_path in waste_type_dir:\n",
    "#         category_name = os.path.basename(category_path)  # Extract category name\n",
    "#         print(f\"Category: {category_name}\")\n",
    "\n",
    "#         # Check for \"default\" and \"real_world\" subfolders\n",
    "#         default_path = os.path.join(category_path, \"default\")\n",
    "#         real_world_path = os.path.join(category_path, \"real_world\")\n",
    "\n",
    "#         if not os.path.exists(default_path) or not os.path.exists(real_world_path):\n",
    "#             print(f\"  Missing subfolders in {category_name}. Expected 'default' and 'real_world'.\")\n",
    "#             continue\n",
    "\n",
    "#         # Count images in each subfolder\n",
    "#         default_images = len([f for f in os.listdir(default_path) if os.path.isfile(os.path.join(default_path, f))])\n",
    "#         real_world_images = len([f for f in os.listdir(real_world_path) if os.path.isfile(os.path.join(real_world_path, f))])\n",
    "\n",
    "#         # Output results\n",
    "#         print(f\"  Default: {default_images} images\")\n",
    "#         print(f\"  Real World: {real_world_images} images\")\n",
    "\n",
    "#         # Verify expected count\n",
    "#         if default_images != 250 or real_world_images != 250:\n",
    "#             print(f\"  WARNING: Expected 250 images in each folder, but found {default_images} (default) and {real_world_images} (real_world).\")\n",
    "\n",
    "# # Run the function\n",
    "# count_images_per_folder(waste_type_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption: All materials are food and beverage free\n",
    "Also..talk about liquid disposal fountain next to the trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_waste = ['coffee_grounds','eggshells', 'food_waste', 'tea_bags']\n",
    "mixed_recycle = ['aluminum_food_cans', 'aluminum_soda_cans', 'aerosol_cans', 'cardboard_boxes', 'cardboard_packaging','glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_soda_bottles', 'plastic_water_bottles','steel_food_cans']\n",
    "trash = ['clothes','styrofoam_cups', 'styrofoam_food_containers', 'disposable_plastic_cutlery', 'paper_cups', 'paper_cup_lids', 'plastic_shopping_bags','plastic_straws', 'plastic_trash_bags', 'shoes' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(food_waste) + len(mixed_recycle) + len(trash) == len(waste_type_dir):\n",
    "    print(\"All categories included!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food Waste Directories: ['waste-images/food_waste', 'waste-images/eggshells', 'waste-images/coffee_grounds', 'waste-images/tea_bags']\n",
      "Mixed Recycle Directories: ['waste-images/office_paper', 'waste-images/glass_food_jars', 'waste-images/aluminum_soda_cans', 'waste-images/magazines', 'waste-images/plastic_soda_bottles', 'waste-images/aerosol_cans', 'waste-images/aluminum_food_cans', 'waste-images/newspaper', 'waste-images/glass_cosmetic_containers', 'waste-images/plastic_water_bottles', 'waste-images/steel_food_cans', 'waste-images/cardboard_packaging', 'waste-images/cardboard_boxes', 'waste-images/glass_beverage_bottles', 'waste-images/plastic_food_containers', 'waste-images/plastic_detergent_bottles']\n",
      "Trash Directories: ['waste-images/disposable_plastic_cutlery', 'waste-images/plastic_shopping_bags', 'waste-images/styrofoam_food_containers', 'waste-images/paper_cups', 'waste-images/plastic_straws', 'waste-images/styrofoam_cups', 'waste-images/shoes', 'waste-images/plastic_trash_bags']\n"
     ]
    }
   ],
   "source": [
    "# Function to map materials to their directories\n",
    "def map_to_directories(materials, directories):\n",
    "    return [dir_path for dir_path in directories if dir_path.split('/')[-1] in materials]\n",
    "\n",
    "# Map each list to directories\n",
    "food_waste_dirs = map_to_directories(food_waste, waste_type_dir)\n",
    "mixed_recycle_dirs = map_to_directories(mixed_recycle, waste_type_dir)\n",
    "trash_dirs = map_to_directories(trash, waste_type_dir)\n",
    "\n",
    "# Print results\n",
    "print(\"Food Waste Directories:\", food_waste_dirs)\n",
    "print(\"Mixed Recycle Directories:\", mixed_recycle_dirs)\n",
    "print(\"Trash Directories:\", trash_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food Waste:  True\n",
      "Mixed Recycling:  True\n",
      "Trash:  True\n"
     ]
    }
   ],
   "source": [
    "# verify if all categories' working directory was matched\n",
    "print(\"Food Waste: \", len(food_waste_dirs)==len(food_waste))\n",
    "print(\"Mixed Recycling: \", len(mixed_recycle_dirs)==len(mixed_recycle))\n",
    "print(\"Trash: \", len(trash_dirs)==len(trash_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to count total images in default and real_world subdirectories for a category\n",
    "def count_total_images(category_dirs):\n",
    "    total_images = 0\n",
    "    for dir_path in category_dirs:\n",
    "\n",
    "        # Count images in default and real_world subdirectories\n",
    "        default_count = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]) if os.path.exists(dir_path) else 0\n",
    "        # Add to total count\n",
    "        total_images += (default_count)\n",
    "    return total_images\n",
    "\n",
    "# Calculate total image counts for each category\n",
    "food_waste_total = count_total_images(food_waste_dirs)\n",
    "mixed_recycle_total = count_total_images(mixed_recycle_dirs)\n",
    "trash_total = count_total_images(trash_dirs)\n",
    "\n",
    "# Calculate total images across all categories\n",
    "overall_total = food_waste_total + mixed_recycle_total + trash_total\n",
    "\n",
    "# Calculate percentages\n",
    "categories = ['Food Waste', 'Mixed Recycle', 'Trash']\n",
    "percentages = [\n",
    "    (food_waste_total / overall_total) * 100,\n",
    "    (mixed_recycle_total / overall_total) * 100,\n",
    "    (trash_total / overall_total) * 100\n",
    "]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(categories, percentages, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.xlabel('Waste Categories')\n",
    "plt.ylabel('Percentage of Total Images (%)')\n",
    "plt.title('Percentage of Images per Waste Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GANs to Create Synthetic Food Waste Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to load and resize images\n",
    "def load_images_from_dirs(dirs, image_size=(64, 64)):\n",
    "    images = []\n",
    "    for dir_path in dirs:\n",
    "        for img_file in os.listdir(dir_path):\n",
    "            img_path = os.path.join(dir_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")  # Ensure all images are RGB\n",
    "                img = img.resize(image_size)  # Resize to target size\n",
    "                images.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    return np.array(images)\n",
    "# Load images for each category\n",
    "food_waste_images = load_images_from_dirs(food_waste_dirs)\n",
    "mixed_recycle_images = load_images_from_dirs(mixed_recycle_dirs)\n",
    "trash_images = load_images_from_dirs(trash_dirs)\n",
    "\n",
    "# Normalize image data (-1 to 1 range) because we are using Tanh activation in the last layer of the model\n",
    "food_waste_images = (food_waste_images / 127.5) - 1.0\n",
    "mixed_recycle_images = (mixed_recycle_images / 127.5) - 1.0\n",
    "trash_images = (trash_images / 127.5) - 1.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Food Waste Images Shape: {food_waste_images.shape}, dtype: {food_waste_images.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to display images\n",
    "def show_images(images, title, n=5):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(n, len(images))):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Show some images from each category\n",
    "show_images(food_waste_images, \"Food Waste Images\")\n",
    "show_images(mixed_recycle_images, \"Mixed Recycle Images\")\n",
    "show_images(trash_images, \"Trash Images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture: \n",
    "* Generator: \n",
    "    * What It Does: Generates synthetic images from random noise.\n",
    "    * Purpose: Learns to create images that are indistinguishable from real ones.\n",
    "    * Why It's Important: Produces new data for augmentation, which helps balance your dataset.Takes random noise as input and generates images.\n",
    "* Discriminator: Takes real or generated images as input and predicts whether they are real or fake.\n",
    "    * What It Does: Classifies images as real or fake.\n",
    "    * Purpose: Helps the generator improve its ability to produce realistic images.\n",
    "    * Why It's Important: Provides feedback to the generator, guiding it towards producing better images.\n",
    "* GAN: Combines the generator and discriminator to train the generator to produce more realistic images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN architecture \n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128 * 16 * 16, activation=\"relu\", input_dim=latent_dim),  # Adjust size\n",
    "        layers.Reshape((16, 16, 128)),  # Adjust to (16, 16, 128)\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),  # Upsample to (32, 32, 128)\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),  # Upsample to (64, 64, 128)\n",
    "        layers.Conv2D(3, kernel_size=7, activation=\"tanh\", padding=\"same\")  # Final output (64, 64, 3)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", input_shape=image_shape),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation=\"sigmoid\")  # Binary classification (real/fake)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "test_noise = tf.random.normal([1, latent_dim])  # Single random noise vector\n",
    "generated_image = generator(test_noise)\n",
    "print(f\"Generator output shape: {generated_image.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step combines the Generator and Discriminator into a complete GAN system so that the Generator can learn through feedback from the Discriminator.\n",
    "* Frozen Discriminator: During GAN training, the discriminator's weights are \"frozen\" (non-trainable). This ensures that only the generator is updated during this phase. The discriminator indirectly trains the generator by providing signals on how well it is \"fooling\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "latent_dim = 100  # Size of noise vector\n",
    "image_shape = (64, 64, 3)  # Image dimensions\n",
    "\n",
    "# Instantiate models\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(image_shape)\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Build and compile GAN\n",
    "discriminator.trainable = False  # Freeze discriminator weights for GAN training\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()  # Prints a textual summary of the generator model\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, to_file=\"generator_architecture.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()  # Prints a textual summary of the discriminator model\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, to_file=\"discriminator_architecture.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.summary()  # Prints a textual summary of the GAN model\n",
    "tf.keras.utils.plot_model(gan, show_shapes=True, to_file=\"gan_architecture.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a GAN involves a balance between improving both the Generator and the Discriminator. Here's how this happens:\n",
    "* Step 1: Train the Discriminator\n",
    "    * Goal: Make the discriminator better at distinguishing real images from fake images.\n",
    "    * Process:\n",
    "        * Select a batch of real images from the dataset.\n",
    "        * Generate a batch of fake images using the generator.\n",
    "        * Train the discriminator on both:\n",
    "        *Real images (label: 1)\n",
    "Fake images (label: 0)\n",
    "Loss Function: Binary cross-entropy, which evaluates how well the discriminator predicts the labels.\n",
    "Step 2: Train the Generator (via the GAN model)\n",
    "Goal: Make the generator produce more realistic images to \"fool\" the discriminator.\n",
    "Process:\n",
    "Generate random noise vectors.\n",
    "Use the generator to create synthetic images.\n",
    "Pass these images through the discriminator (via the GAN model).\n",
    "Train the generator with a label of 1 (it tries to make the discriminator believe the fake images are real).\n",
    "Why Is the Discriminator Frozen? The generator relies on the discriminator's feedback to improve, but we do not want the discriminator to learn during this step. The focus is solely on improving the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_waste_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_gan(generator, discriminator, gan, real_images, epochs=10000, batch_size=64):\n",
    "    latent_dim = generator.input_shape[1]\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        try:\n",
    "            # Train discriminator\n",
    "            idx = np.random.randint(0, real_images.shape[0], half_batch)\n",
    "            real_samples = real_images[idx]\n",
    "            noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "            fake_samples = generator.predict(noise)\n",
    "            real_labels = np.ones((half_batch, 1))\n",
    "            fake_labels = np.zeros((half_batch, 1))\n",
    "\n",
    "            # Debugging shapes\n",
    "            print(f\"Epoch {epoch}: Real {real_samples.shape}, Fake {fake_samples.shape}\")\n",
    "            print(f\"Real labels: {real_labels.shape}, Fake labels: {fake_labels.shape}\")\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "\n",
    "            # Train generator (via GAN model)\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            misleading_labels = np.ones((batch_size, 1))\n",
    "            g_loss = gan.train_on_batch(noise, misleading_labels)\n",
    "\n",
    "            # Print progress every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | D Loss: {d_loss_real[0] + d_loss_fake[0]} | G Loss: {g_loss}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error at epoch {epoch}: {e}\")\n",
    "\n",
    "# Train GAN using food waste images\n",
    "train_gan(generator, discriminator, gan, food_waste_images, epochs=5000, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images\n",
    "noise = np.random.normal(0, 1, (10, latent_dim))  # Generate 10 samples\n",
    "synthetic_images = generator.predict(noise)\n",
    "\n",
    "# Save generated images\n",
    "for i, img in enumerate(synthetic_images):\n",
    "    img = (img * 255).astype(np.uint8)  # Rescale to 0-255\n",
    "    Image.fromarray(img).save(f\"synthetic_image_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images\n",
    "noise = np.random.normal(0, 1, (10, latent_dim))  # Generate 10 samples\n",
    "synthetic_images = generator.predict(noise)\n",
    "\n",
    "# Save generated images\n",
    "for i, img in enumerate(synthetic_images):\n",
    "    img = (img * 255).astype(np.uint8)  # Rescale to 0-255\n",
    "    Image.fromarray(img).save(f\"synthetic_image_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building -- (GPT reccomended creating genators for the train and val data before)\n",
    "\n",
    "* Convolutional Layers (Conv2D): Extract features from images by applying filters.\n",
    "* MaxPooling Layers (MaxPooling2D): Reduce the spatial dimensions of feature maps to focus on key features.\n",
    "* Dropout Layers: Reduce overfitting by randomly disabling neurons during training.\n",
    "* Fully Connected Layer (Dense): Combines all extracted features to classify the images.\n",
    "* Output Layer: The softmax activation ensures probabilities for the 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# model = Sequential([\n",
    "#     # First convolutional block\n",
    "#     Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),  # 32 filters, 3x3 kernel\n",
    "#     MaxPooling2D(pool_size=(2, 2)),  # Downsample feature maps by 2x2\n",
    "#     Dropout(0.25),  # Randomly disable 25% of neurons to prevent overfitting\n",
    "\n",
    "#     # Second convolutional block\n",
    "#     Conv2D(64, (3, 3), activation='relu'),  # 64 filters, 3x3 kernel\n",
    "#     MaxPooling2D(pool_size=(2, 2)),  # Downsample again\n",
    "#     Dropout(0.25),\n",
    "\n",
    "#     # Flatten and fully connected layers\n",
    "#     Flatten(),  # Flatten the 2D fpeature maps into 1D\n",
    "#     Dense(128, activation='relu'),  # Fully connected layer with 128 neurons\n",
    "#     Dropout(0.5),  # 50% dropout for regularization\n",
    "\n",
    "#     # Output layer\n",
    "#     Dense(3, activation='softmax')  # Final layer for 3 categories (food waste, mixed recycling, trash)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/loubenskybelile/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /Users/loubenskybelile/anaconda3/lib/python3.10/site-packages (3.1.43)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /Users/loubenskybelile/anaconda3/lib/python3.10/site-packages (75.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/loubenskybelile/anaconda3/lib/python3.10/site-packages (from gitpython>=3.1.30) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/loubenskybelile/anaconda3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (5.0.1)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 2.2s, installed 2 packages: ['gitpython>=3.1.30', 'setuptools>=70.0.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2024-12-8 Python-3.9.6 torch-2.5.1 CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:01<00:00, 11.1MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Detections:\n",
      "        xmin       ymin        xmax        ymax  confidence  class  name\n",
      "0  82.202858  49.422218  181.709503  197.195633    0.668750     41   cup\n",
      "1   0.131876  84.751144   56.802284  196.373810    0.333541     45  bowl\n",
      "Filtered detections saved to filtered_detections.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Load the YOLOv5 model (pre-trained on COCO dataset)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Use 'yolov5s' for a small, fast model\n",
    "\n",
    "# Path to the input image (replace with your image path)\n",
    "image_path = 'waste-images/paper_cups/default_Image_186.png'\n",
    "\n",
    "# Debug: Check the input image\n",
    "img = Image.open(image_path)\n",
    "img.show() \n",
    "\n",
    "# Run inference\n",
    "results = model(image_path)  # Perform object detection\n",
    "\n",
    "# Get detections as a Pandas DataFrame\n",
    "detections = results.pandas().xyxy[0]\n",
    "\n",
    "# Filter detections by confidence threshold (e.g., 0.25)\n",
    "confidence_threshold = 0.25\n",
    "filtered_detections = detections[detections['confidence'] > confidence_threshold]\n",
    "\n",
    "# Display filtered detections\n",
    "if filtered_detections.empty:\n",
    "    print(\"No objects detected above the confidence threshold.\")\n",
    "else:\n",
    "    print(\"Filtered Detections:\")\n",
    "    print(filtered_detections)\n",
    "\n",
    "# Save detections to a CSV file\n",
    "filtered_detections.to_csv('filtered_detections.csv', index=False)\n",
    "print(\"Filtered detections saved to filtered_detections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the paths for each category\n",
    "food_waste_dirs = ['waste-images/food_waste', 'waste-images/eggshells', 'waste-images/coffee_grounds', 'waste-images/tea_bags']\n",
    "mixed_recycle_dirs = ['waste-images/office_paper', 'waste-images/glass_food_jars', 'waste-images/aluminum_soda_cans', \n",
    "                      'waste-images/magazines', 'waste-images/plastic_soda_bottles', 'waste-images/aerosol_cans',\n",
    "                      'waste-images/aluminum_food_cans', 'waste-images/newspaper', 'waste-images/glass_cosmetic_containers',\n",
    "                      'waste-images/plastic_water_bottles', 'waste-images/steel_food_cans', 'waste-images/cardboard_packaging',\n",
    "                      'waste-images/cardboard_boxes', 'waste-images/glass_beverage_bottles', 'waste-images/plastic_food_containers',\n",
    "                      'waste-images/plastic_detergent_bottles']\n",
    "trash_dirs = ['waste-images/disposable_plastic_cutlery', 'waste-images/plastic_shopping_bags', 'waste-images/styrofoam_food_containers', \n",
    "              'waste-images/paper_cups', 'waste-images/plastic_straws', 'waste-images/styrofoam_cups', 'waste-images/shoes', \n",
    "              'waste-images/plastic_trash_bags']\n",
    "\n",
    "# Combine into categories\n",
    "categories = {\n",
    "    'Food Waste': food_waste_dirs,\n",
    "    'Mixed Recycle': mixed_recycle_dirs,\n",
    "    'Trash': trash_dirs\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "train_dir = 'waste-images/train'\n",
    "val_dir = 'waste-images/val'\n",
    "test_dir = 'waste-images/test'\n",
    "split_ratios = (0.7, 0.2, 0.1)  # Train, validation, and test ratios\n",
    "\n",
    "# Create train, validation, and test directories\n",
    "for output_dir in [train_dir, val_dir, test_dir]:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for category in categories.keys():\n",
    "        os.makedirs(os.path.join(output_dir, category), exist_ok=True)\n",
    "\n",
    "# Process and split data\n",
    "for category, subcategories in categories.items():\n",
    "    for sub_dir in subcategories:\n",
    "        if not os.path.exists(sub_dir):\n",
    "            print(f\"Warning: Subcategory folder '{sub_dir}' does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get list of images\n",
    "        images = [os.path.join(sub_dir, img) for img in os.listdir(sub_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "        # Split images into train, validation, and test sets\n",
    "        train_images, temp_images = train_test_split(images, train_size=split_ratios[0], random_state=42)\n",
    "        val_images, test_images = train_test_split(temp_images, test_size=split_ratios[2] / (split_ratios[1] + split_ratios[2]), random_state=42)\n",
    "\n",
    "        # Copy images to the appropriate directories\n",
    "        train_sub_dir = os.path.join(train_dir, category)\n",
    "        val_sub_dir = os.path.join(val_dir, category)\n",
    "        test_sub_dir = os.path.join(test_dir, category)\n",
    "\n",
    "        for img_path in train_images:\n",
    "            shutil.copy(img_path, train_sub_dir)\n",
    "        for img_path in val_images:\n",
    "            shutil.copy(img_path, val_sub_dir)\n",
    "        for img_path in test_images:\n",
    "            shutil.copy(img_path, test_sub_dir)\n",
    "\n",
    "        print(f\"Processed {len(images)} images from '{sub_dir}' into '{category}'.\")\n",
    "\n",
    "print(\"Folder creation and image splitting completed successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory contents: ['Food Waste', 'Trash', 'Mixed Recycle']\n",
      "Validation directory contents: ['Food Waste', 'Trash', 'Mixed Recycle']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Train directory contents:\", os.listdir(train_dir))\n",
    "print(\"Validation directory contents:\", os.listdir(val_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 181ms/step - accuracy: 0.4006 - loss: 1.1151 - val_accuracy: 0.5033 - val_loss: 0.9949\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 250ms/step - accuracy: 0.5717 - loss: 0.9224 - val_accuracy: 0.5867 - val_loss: 0.8574\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 201ms/step - accuracy: 0.6318 - loss: 0.8480 - val_accuracy: 0.7133 - val_loss: 0.6631\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 213ms/step - accuracy: 0.6697 - loss: 0.7845 - val_accuracy: 0.6933 - val_loss: 0.7635\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.7329 - loss: 0.6845 - val_accuracy: 0.7767 - val_loss: 0.5642\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.7420 - loss: 0.6594 - val_accuracy: 0.7467 - val_loss: 0.6190\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.7660 - loss: 0.6123 - val_accuracy: 0.7500 - val_loss: 0.6332\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - accuracy: 0.7390 - loss: 0.6356 - val_accuracy: 0.7967 - val_loss: 0.5447\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 0.7763 - loss: 0.5692 - val_accuracy: 0.8167 - val_loss: 0.5227\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 0.7364 - loss: 0.6453 - val_accuracy: 0.7767 - val_loss: 0.5426\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.7706 - loss: 0.5703 - val_accuracy: 0.8200 - val_loss: 0.4913\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - accuracy: 0.7777 - loss: 0.5403 - val_accuracy: 0.7833 - val_loss: 0.5423\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.7960 - loss: 0.5355 - val_accuracy: 0.8067 - val_loss: 0.4983\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 164ms/step - accuracy: 0.7929 - loss: 0.5344 - val_accuracy: 0.8200 - val_loss: 0.5086\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.7964 - loss: 0.5176 - val_accuracy: 0.7833 - val_loss: 0.5400\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 0.7979 - loss: 0.5373 - val_accuracy: 0.8367 - val_loss: 0.4390\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.8029 - loss: 0.4874 - val_accuracy: 0.7867 - val_loss: 0.4893\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.7713 - loss: 0.5453 - val_accuracy: 0.8233 - val_loss: 0.4610\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - accuracy: 0.7942 - loss: 0.5185 - val_accuracy: 0.8333 - val_loss: 0.4334\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - accuracy: 0.8150 - loss: 0.4501 - val_accuracy: 0.8433 - val_loss: 0.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model architecture\n",
    "def build_waste_classification_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Convolutional Layers\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Data augmentation\n",
    "def create_data_generators(train_dir, val_dir, image_size, batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Define parameters\n",
    "input_shape = (64, 64, 3)  # Image size (height, width, channels)\n",
    "num_classes = 3  # Food Waste, Mixed Recycle, Trash\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "train_dir = 'waste-images/train'\n",
    "val_dir = 'waste-images/val'\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = build_waste_classification_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate data\n",
    "train_generator, val_generator = create_data_generators(train_dir, val_dir, (64, 64), batch_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('waste_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.3686 - loss: 1.1560 - val_accuracy: 0.4200 - val_loss: 1.0450\n",
      "Epoch 2/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.4280 - loss: 1.0472 - val_accuracy: 0.5833 - val_loss: 0.9312\n",
      "Epoch 3/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.6018 - loss: 0.9062 - val_accuracy: 0.6233 - val_loss: 0.8257\n",
      "Epoch 4/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.6468 - loss: 0.8604 - val_accuracy: 0.6467 - val_loss: 0.7547\n",
      "Epoch 5/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.6770 - loss: 0.7934 - val_accuracy: 0.7233 - val_loss: 0.6485\n",
      "Epoch 6/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.7261 - loss: 0.6823 - val_accuracy: 0.7333 - val_loss: 0.6639\n",
      "Epoch 7/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.7307 - loss: 0.6670 - val_accuracy: 0.7700 - val_loss: 0.6059\n",
      "Epoch 8/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.7442 - loss: 0.6199 - val_accuracy: 0.7767 - val_loss: 0.5436\n",
      "Epoch 9/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.7248 - loss: 0.7126 - val_accuracy: 0.8033 - val_loss: 0.5320\n",
      "Epoch 10/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.7836 - loss: 0.5701 - val_accuracy: 0.8333 - val_loss: 0.4729\n",
      "Epoch 11/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.7719 - loss: 0.5962 - val_accuracy: 0.7567 - val_loss: 0.5858\n",
      "Epoch 12/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.7652 - loss: 0.5836 - val_accuracy: 0.7967 - val_loss: 0.5076\n",
      "Epoch 13/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.7777 - loss: 0.5752 - val_accuracy: 0.8233 - val_loss: 0.4701\n",
      "Epoch 14/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - accuracy: 0.7953 - loss: 0.5430 - val_accuracy: 0.8267 - val_loss: 0.5356\n",
      "Epoch 15/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.7869 - loss: 0.5894 - val_accuracy: 0.7767 - val_loss: 0.5184\n",
      "Epoch 16/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7913 - loss: 0.5198 - val_accuracy: 0.8267 - val_loss: 0.4550\n",
      "Epoch 17/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - accuracy: 0.8041 - loss: 0.5241 - val_accuracy: 0.8000 - val_loss: 0.5380\n",
      "Epoch 18/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8195 - loss: 0.4850 - val_accuracy: 0.8333 - val_loss: 0.4694\n",
      "Epoch 19/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8109 - loss: 0.4823 - val_accuracy: 0.8300 - val_loss: 0.4525\n",
      "Epoch 20/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.8113 - loss: 0.4441 - val_accuracy: 0.8633 - val_loss: 0.4066\n",
      "Epoch 21/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.7985 - loss: 0.5187 - val_accuracy: 0.8367 - val_loss: 0.4353\n",
      "Epoch 22/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.7982 - loss: 0.5048 - val_accuracy: 0.8167 - val_loss: 0.5196\n",
      "Epoch 23/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8105 - loss: 0.4864 - val_accuracy: 0.8333 - val_loss: 0.4501\n",
      "Epoch 24/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8125 - loss: 0.4756 - val_accuracy: 0.8267 - val_loss: 0.4507\n",
      "Epoch 25/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8165 - loss: 0.4734 - val_accuracy: 0.7567 - val_loss: 0.5743\n",
      "Epoch 26/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8079 - loss: 0.4790 - val_accuracy: 0.8500 - val_loss: 0.4142\n",
      "Epoch 27/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8345 - loss: 0.4073 - val_accuracy: 0.8500 - val_loss: 0.4039\n",
      "Epoch 28/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8286 - loss: 0.4703 - val_accuracy: 0.8600 - val_loss: 0.4279\n",
      "Epoch 29/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.8289 - loss: 0.4730 - val_accuracy: 0.8800 - val_loss: 0.3776\n",
      "Epoch 30/30\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.8224 - loss: 0.4760 - val_accuracy: 0.8300 - val_loss: 0.5136\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model architecture\n",
    "def build_waste_classification_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Convolutional Layers\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.7),\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Data augmentation\n",
    "def create_data_generators(train_dir, val_dir, image_size, batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Define parameters\n",
    "input_shape = (64, 64, 3)  # Image size (height, width, channels)\n",
    "num_classes = 3  # Food Waste, Mixed Recycle, Trash\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "train_dir = 'waste-images/train'\n",
    "val_dir = 'waste-images/val'\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = build_waste_classification_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate data\n",
    "train_generator, val_generator = create_data_generators(train_dir, val_dir, (64, 64), batch_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 3 classes.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9076 - loss: 0.2779\n",
      "Test Accuracy: 0.8933333158493042\n",
      "Test Loss: 0.296490341424942\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the test directory\n",
    "test_dir = 'waste-images/test'  # Replace with your test directory path\n",
    "\n",
    "# Create the test data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step\n",
      "[[46  1  3]\n",
      " [ 3 46  1]\n",
      " [ 5  3 42]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Food Waste       0.85      0.92      0.88        50\n",
      "Mixed Recycle       0.92      0.92      0.92        50\n",
      "        Trash       0.91      0.84      0.88        50\n",
      "\n",
      "     accuracy                           0.89       150\n",
      "    macro avg       0.89      0.89      0.89       150\n",
      " weighted avg       0.89      0.89      0.89       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(model.predict(test_generator), axis=-1)\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get class indices from the test generator\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Actual classes\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Get file paths for test images\n",
    "test_images = test_generator.filepaths\n",
    "\n",
    "# Plot some images with their predicted and actual labels\n",
    "def plot_predictions(images, true_labels, predicted_labels, class_labels, num_images=10):\n",
    "    plt.figure(figsize=(15, num_images * 3))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_images // 5 + 1, 5, i + 1)\n",
    "        img = plt.imread(images[i])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\n",
    "            f\"True: {class_labels[true_labels[i]]}\\nPred: {class_labels[predicted_labels[i]]}\",\n",
    "            color=\"green\" if true_labels[i] == predicted_labels[i] else \"red\"\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_predictions(test_images, true_classes, predicted_classes, class_labels, num_images=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get class indices from the test generator\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Actual classes\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Get file paths for test images\n",
    "test_images = test_generator.filepaths\n",
    "\n",
    "# Function to plot images with predictions\n",
    "def plot_all_predictions(images, true_labels, predicted_labels, class_labels, batch_size=20):\n",
    "    num_images = len(images)\n",
    "    plt.figure(figsize=(15, batch_size // 5 * 3))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot((num_images // 5) + 1, 5, i + 1)\n",
    "        img = plt.imread(images[i])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\n",
    "            f\"True: {class_labels[true_labels[i]]}\\nPred: {class_labels[predicted_labels[i]]}\",\n",
    "            color=\"green\" if true_labels[i] == predicted_labels[i] else \"red\",\n",
    "            fontsize=10\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot all test images\n",
    "plot_all_predictions(test_images, true_classes, predicted_classes, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "waste-images/test/Mixed Recycle/default_Image_112.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_112.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_103.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_103.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_12.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_12.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_136.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_136.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_111.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_111.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_214.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_214.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_42.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_42.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_223.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_223.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_241.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_241.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_21.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_21.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_127.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_127.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_118.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_118.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_98.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_98.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_67.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_67.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_248.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_248.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_106.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_106.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_207.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_207.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_65.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_65.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_236.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_236.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_45.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_45.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_82.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_82.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_65.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_65.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_200.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_200.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_132.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_132.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_219.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_219.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_67.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_67.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_127.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_127.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_241.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_241.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_222.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_222.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_157.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_157.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_200.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_200.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_111.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_111.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_13.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_13.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_79.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_79.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_155.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_155.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_86.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_86.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_106.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_106.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_65.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_65.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_157.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_157.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_75.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_75.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_223.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_223.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_82.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_82.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_6.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_6.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_231.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_231.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_161.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_161.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_217.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_217.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_248.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_248.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_132.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_132.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_75.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_75.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_98.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_98.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_127.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_127.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_106.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_106.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_25.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_25.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_110.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_110.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_87.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_87.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_207.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_207.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_241.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_241.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_42.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_42.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_21.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_21.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_103.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_103.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_157.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_157.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_111.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_111.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_222.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_222.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_241.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_241.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_82.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_82.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_200.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_200.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_207.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_207.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_127.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_127.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_86.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_86.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_236.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_236.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_6.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_6.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_13.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_13.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_6.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_6.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_139.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_139.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_231.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_231.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_155.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_155.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_229.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_229.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_118.png",
           "Trash",
           "<img src='waste-images/test/Trash/real_world_Image_118.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_67.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_67.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_230.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_230.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_75.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/real_world_Image_75.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_3.png",
           "Food Waste",
           "<img src='waste-images/test/Food Waste/default_Image_3.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_131.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_131.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_174.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_174.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_217.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_217.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_86.png",
           "Trash",
           "<img src='waste-images/test/Trash/default_Image_86.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_79.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Mixed Recycle/default_Image_79.png' style='max-width:150px;'>"
          ]
         ],
         "hovertemplate": "%{customdata[0]}<br>True: %{customdata[1]}<br>Pred: %{customdata[2]}<br>Conf: %{customdata[3]}",
         "legendgroup": "True",
         "marker": {
          "color": "#636efa",
          "size": 10,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "True",
         "orientation": "h",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.5950714349746704,
          0.9815120697021484,
          0.9979695677757263,
          0.4426749646663666,
          0.9945650100708008,
          0.9810883402824402,
          0.9413434267044067,
          0.47403615713119507,
          0.9965227842330933,
          0.8219415545463562,
          0.983411967754364,
          0.662485659122467,
          0.9999281167984009,
          0.9987128973007202,
          0.9276333451271057,
          0.4240116477012634,
          0.7093544006347656,
          0.9988812804222107,
          0.8486422300338745,
          0.8073220252990723,
          0.9657003283500671,
          0.9989501237869263,
          0.7417657375335693,
          0.9999182820320129,
          0.746491551399231,
          0.9995230436325073,
          0.9982622265815735,
          0.6141789555549622,
          0.9996021389961243,
          0.9758277535438538,
          0.9933035373687744,
          0.8536605834960938,
          0.9999293088912964,
          0.5056942701339722,
          0.9985435009002686,
          0.7844541072845459,
          0.980270266532898,
          0.9832441210746765,
          0.9382714629173279,
          0.9985962510108948,
          0.8297489285469055,
          0.49507176876068115,
          0.6524668335914612,
          0.9872813820838928,
          0.8974716663360596,
          0.8536605834960938,
          0.6019957065582275,
          0.9941423535346985,
          0.8622615933418274,
          0.9137060046195984,
          0.5569564700126648,
          0.9545870423316956,
          0.9964699745178223,
          0.9796443581581116,
          0.9999532699584961,
          0.6430058479309082,
          0.9391841292381287,
          0.9445590376853943,
          0.948768138885498,
          0.6995962858200073,
          0.9865245819091797,
          0.9676055908203125,
          0.8222678303718567,
          0.9994548559188843,
          0.9987128973007202,
          0.9863629937171936,
          0.9435905814170837,
          0.9999590516090393,
          0.638257622718811,
          0.9521337747573853,
          0.7353328466415405,
          0.9582250714302063,
          0.8882668018341064,
          0.9412376284599304,
          0.9949192404747009,
          0.9959498047828674,
          0.9750529527664185,
          0.4841279089450836,
          0.9631473422050476,
          0.904144287109375,
          0.9004396796226501,
          0.9730657339096069,
          0.9942082762718201,
          0.9999532699584961,
          0.9137060046195984,
          0.9983766674995422,
          0.6754864454269409
         ],
         "xaxis": "x",
         "y": [
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Trash",
          "Food Waste",
          "Food Waste",
          "Trash",
          "Trash",
          "Food Waste",
          "Food Waste",
          "Food Waste",
          "Mixed Recycle",
          "Mixed Recycle",
          "Food Waste",
          "Food Waste",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle",
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Food Waste",
          "Food Waste",
          "Food Waste",
          "Trash",
          "Trash",
          "Trash",
          "Food Waste",
          "Mixed Recycle",
          "Food Waste",
          "Mixed Recycle",
          "Trash",
          "Food Waste",
          "Mixed Recycle",
          "Mixed Recycle",
          "Mixed Recycle",
          "Food Waste",
          "Food Waste",
          "Trash",
          "Food Waste",
          "Mixed Recycle",
          "Mixed Recycle",
          "Food Waste",
          "Trash",
          "Mixed Recycle",
          "Food Waste",
          "Trash",
          "Trash",
          "Food Waste",
          "Food Waste",
          "Mixed Recycle",
          "Mixed Recycle",
          "Trash",
          "Trash",
          "Food Waste",
          "Trash",
          "Trash",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle",
          "Food Waste",
          "Trash",
          "Food Waste",
          "Food Waste",
          "Food Waste",
          "Trash",
          "Mixed Recycle",
          "Mixed Recycle",
          "Trash",
          "Food Waste",
          "Mixed Recycle",
          "Food Waste",
          "Food Waste",
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Mixed Recycle"
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "waste-images/test/Trash/real_world_Image_223.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Trash/real_world_Image_223.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/real_world_Image_184.png",
           "Trash",
           "<img src='waste-images/test/Mixed Recycle/real_world_Image_184.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_236.png",
           "Food Waste",
           "<img src='waste-images/test/Trash/default_Image_236.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_174.png",
           "Trash",
           "<img src='waste-images/test/Food Waste/default_Image_174.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_131.png",
           "Food Waste",
           "<img src='waste-images/test/Trash/real_world_Image_131.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_230.png",
           "Food Waste",
           "<img src='waste-images/test/Trash/real_world_Image_230.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/default_Image_45.png",
           "Food Waste",
           "<img src='waste-images/test/Trash/default_Image_45.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/real_world_Image_219.png",
           "Mixed Recycle",
           "<img src='waste-images/test/Food Waste/real_world_Image_219.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Trash/real_world_Image_229.png",
           "Food Waste",
           "<img src='waste-images/test/Trash/real_world_Image_229.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Food Waste/default_Image_103.png",
           "Trash",
           "<img src='waste-images/test/Food Waste/default_Image_103.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_132.png",
           "Food Waste",
           "<img src='waste-images/test/Mixed Recycle/default_Image_132.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_136.png",
           "Food Waste",
           "<img src='waste-images/test/Mixed Recycle/default_Image_136.png' style='max-width:150px;'>"
          ],
          [
           "waste-images/test/Mixed Recycle/default_Image_214.png",
           "Food Waste",
           "<img src='waste-images/test/Mixed Recycle/default_Image_214.png' style='max-width:150px;'>"
          ]
         ],
         "hovertemplate": "%{customdata[0]}<br>True: %{customdata[1]}<br>Pred: %{customdata[2]}<br>Conf: %{customdata[3]}",
         "legendgroup": "False",
         "marker": {
          "color": "#EF553B",
          "size": 10,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "False",
         "orientation": "h",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.4967074394226074,
          0.5007262229919434,
          0.8333101272583008,
          0.4936063885688782,
          0.6235647797584534,
          0.5188461542129517,
          0.9134507179260254,
          0.42923226952552795,
          0.904005229473114,
          0.4286559224128723,
          0.8014531135559082,
          0.6539926528930664,
          0.8997233510017395
         ],
         "xaxis": "x",
         "y": [
          "Trash",
          "Mixed Recycle",
          "Trash",
          "Food Waste",
          "Trash",
          "Trash",
          "Trash",
          "Food Waste",
          "Trash",
          "Food Waste",
          "Mixed Recycle",
          "Mixed Recycle",
          "Mixed Recycle"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Correct"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Predictions"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Confidence"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "True Label"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Get class labels and predictions\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "data = pd.DataFrame({\n",
    "    'Image Path': test_generator.filepaths,\n",
    "    'True Label': [class_labels[i] for i in test_generator.classes],\n",
    "    'Predicted Label': [class_labels[i] for i in predicted_classes],\n",
    "    'Correct': predicted_classes == test_generator.classes\n",
    "})\n",
    "\n",
    "# Add confidence scores for predictions\n",
    "data['Confidence'] = predictions.max(axis=1)\n",
    "\n",
    "# Function to display interactive scrollable visualization\n",
    "def display_scrollable_predictions(data, num_images=20):\n",
    "    # Select a subset for display (or the entire dataset)\n",
    "    display_data = data.sample(n=num_images)  # Random sample of predictions\n",
    "    display_data['Image'] = display_data['Image Path'].apply(lambda path: f\"<img src='{path}' style='max-width:150px;'>\")\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        display_data,\n",
    "        x='Confidence',\n",
    "        y='True Label',\n",
    "        color='Correct',\n",
    "        hover_data={\n",
    "            'Image Path': False,\n",
    "            'True Label': True,\n",
    "            'Predicted Label': True,\n",
    "            'Confidence': ':.2f',\n",
    "            'Image': True\n",
    "        },\n",
    "        title='Model Predictions'\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10), hovertemplate=\"%{customdata[0]}<br>True: %{customdata[1]}<br>Pred: %{customdata[2]}<br>Conf: %{customdata[3]}\")\n",
    "    fig.show()\n",
    "\n",
    "# Display predictions\n",
    "display_scrollable_predictions(data, num_images=100)  # Customize num_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "# Get class labels and predictions\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a DataFrame for organization\n",
    "data = pd.DataFrame({\n",
    "    'Image Path': test_generator.filepaths,\n",
    "    'True Label': [class_labels[i] for i in test_generator.classes],\n",
    "    'Predicted Label': [class_labels[i] for i in predicted_classes],\n",
    "    'Confidence': predictions.max(axis=1),\n",
    "    'Correct': predicted_classes == test_generator.classes\n",
    "})\n",
    "\n",
    "# Helper function to encode images in base64 for HTML display\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Add the base64-encoded image to the DataFrame for display\n",
    "data['Image HTML'] = data['Image Path'].apply(\n",
    "    lambda path: f\"<img src='data:image/jpeg;base64,{encode_image(path)}' style='width:100px;height:100px;'>\"\n",
    ")\n",
    "\n",
    "# Group data by categories\n",
    "def generate_grouped_html(data):\n",
    "    html = \"<h1>Model Predictions Grouped by Category</h1>\"\n",
    "    for category in class_labels:\n",
    "        html += f\"<h2>{category}</h2>\"\n",
    "        subset = data[data['True Label'] == category]\n",
    "        if subset.empty:\n",
    "            html += \"<p>No images in this category.</p>\"\n",
    "        else:\n",
    "            html += subset[['Image HTML', 'True Label', 'Predicted Label', 'Confidence', 'Correct']].to_html(\n",
    "                escape=False,\n",
    "                index=False\n",
    "            )\n",
    "    return html\n",
    "\n",
    "# Display predictions grouped by category\n",
    "html_content = generate_grouped_html(data)\n",
    "display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image detection Attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Pre-trained on COCO\n",
    "\n",
    "# Path to input image\n",
    "image_path = '/mnt/data/real_world_Image_223.png'\n",
    "\n",
    "# Debug: Check the input image\n",
    "img = Image.open(image_path)\n",
    "img.show()\n",
    "\n",
    "# Run inference with a lower confidence threshold\n",
    "results = model(image_path, conf=0.25)  # Reduce confidence threshold to 25%\n",
    "\n",
    "# Display results\n",
    "results.show()  # Opens image with bounding boxes\n",
    "\n",
    "# Check if detections are present\n",
    "detections = results.pandas().xyxy[0]\n",
    "if detections.empty:\n",
    "    print(\"No objects detected.\")\n",
    "else:\n",
    "    print(\"Detections:\")\n",
    "    print(detections)\n",
    "\n",
    "# Save detections to a CSV file\n",
    "detections.to_csv('detections.csv', index=False)\n",
    "print(\"Detections saved to detections.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "Evaluating Baseline Model...\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9076 - loss: 0.2779\n",
      "Baseline Model - Test Accuracy: 0.8933333158493042\n",
      "Baseline Model - Test Loss: 0.296490341424942\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step\n",
      "Baseline Model - Confusion Matrix:\n",
      "[[46  1  3]\n",
      " [ 3 46  1]\n",
      " [ 5  3 42]]\n",
      "Baseline Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Food Waste       0.85      0.92      0.88        50\n",
      "Mixed Recycle       0.92      0.92      0.92        50\n",
      "        Trash       0.91      0.84      0.88        50\n",
      "\n",
      "     accuracy                           0.89       150\n",
      "    macro avg       0.89      0.89      0.89       150\n",
      " weighted avg       0.89      0.89      0.89       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define paths\n",
    "train_dir = 'waste-images/train'  # Update this path\n",
    "val_dir = 'waste-images/val'      # Update this path\n",
    "test_dir = 'waste-images/test'    # Update this path\n",
    "base_model_path = 'waste_classification_model.h5'  # Path to your base model\n",
    "\n",
    "# Define parameters\n",
    "image_size = (64, 64)\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "epochs = 20\n",
    "\n",
    "# Load the base model\n",
    "base_model = load_model(base_model_path)\n",
    "\n",
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Visualization Function\n",
    "def plot_history(history, experiment_name):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{experiment_name}: Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{experiment_name}: Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, experiment_name):\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "    print(f\"{experiment_name} - Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"{experiment_name} - Test Loss: {test_loss}\")\n",
    "\n",
    "    # Confusion Matrix and Classification Report\n",
    "    test_labels = test_generator.classes\n",
    "    predictions = model.predict(test_generator)\n",
    "    predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "    conf_matrix = confusion_matrix(test_labels, predicted_classes)\n",
    "    print(f\"{experiment_name} - Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    report = classification_report(test_labels, predicted_classes, target_names=list(test_generator.class_indices.keys()))\n",
    "    print(f\"{experiment_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# -------------------- EXPERIMENT 1: Baseline Evaluation --------------------\n",
    "print(\"Evaluating Baseline Model...\")\n",
    "evaluate_model(base_model, \"Baseline Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2: Hyperparameter Tuning\n",
      "Training model with dropout=0.3 and learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - accuracy: 0.4557 - loss: 2.7502 - val_accuracy: 0.4700 - val_loss: 1.1982\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - accuracy: 0.6007 - loss: 0.9369 - val_accuracy: 0.4567 - val_loss: 1.7159\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6648 - loss: 0.8605 - val_accuracy: 0.4467 - val_loss: 1.2631\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.6999 - loss: 0.7432 - val_accuracy: 0.5600 - val_loss: 1.3744\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.6982 - loss: 0.7696 - val_accuracy: 0.4367 - val_loss: 2.6564\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6994 - loss: 0.7663 - val_accuracy: 0.4867 - val_loss: 2.1679\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.7465 - loss: 0.6789 - val_accuracy: 0.6467 - val_loss: 1.2531\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7231 - loss: 0.6650 - val_accuracy: 0.5400 - val_loss: 2.4334\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7356 - loss: 0.6637 - val_accuracy: 0.6433 - val_loss: 1.8167\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.7625 - loss: 0.6432 - val_accuracy: 0.6767 - val_loss: 1.4267\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7511 - loss: 0.6346 - val_accuracy: 0.6667 - val_loss: 1.8635\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7575 - loss: 0.6047 - val_accuracy: 0.7567 - val_loss: 1.0885\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.7606 - loss: 0.6113 - val_accuracy: 0.7467 - val_loss: 1.6761\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7909 - loss: 0.5933 - val_accuracy: 0.6300 - val_loss: 1.7364\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 208ms/step - accuracy: 0.7667 - loss: 0.5893 - val_accuracy: 0.7800 - val_loss: 0.6674\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.8163 - loss: 0.4881 - val_accuracy: 0.7967 - val_loss: 0.6433\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7976 - loss: 0.5795 - val_accuracy: 0.6133 - val_loss: 1.4698\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7718 - loss: 0.5897 - val_accuracy: 0.8233 - val_loss: 0.5076\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 202ms/step - accuracy: 0.7642 - loss: 0.5656 - val_accuracy: 0.7733 - val_loss: 0.9447\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 203ms/step - accuracy: 0.7689 - loss: 0.5859 - val_accuracy: 0.5467 - val_loss: 2.3433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8233333230018616\n",
      "Training model with dropout=0.3 and learning_rate=0.0005\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 217ms/step - accuracy: 0.3859 - loss: 2.7257 - val_accuracy: 0.4600 - val_loss: 1.2345\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 256ms/step - accuracy: 0.5391 - loss: 1.0683 - val_accuracy: 0.4100 - val_loss: 2.0453\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.6061 - loss: 0.8820 - val_accuracy: 0.4867 - val_loss: 2.2086\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 206ms/step - accuracy: 0.6408 - loss: 0.8259 - val_accuracy: 0.3567 - val_loss: 2.9242\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - accuracy: 0.6080 - loss: 0.8809 - val_accuracy: 0.3633 - val_loss: 3.1118\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 228ms/step - accuracy: 0.6638 - loss: 0.7801 - val_accuracy: 0.4533 - val_loss: 1.8905\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 215ms/step - accuracy: 0.6596 - loss: 0.8154 - val_accuracy: 0.6400 - val_loss: 1.1682\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 203ms/step - accuracy: 0.7191 - loss: 0.6619 - val_accuracy: 0.6333 - val_loss: 1.1982\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 214ms/step - accuracy: 0.7164 - loss: 0.7178 - val_accuracy: 0.7000 - val_loss: 1.1801\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 237ms/step - accuracy: 0.6949 - loss: 0.6914 - val_accuracy: 0.6033 - val_loss: 1.8396\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 205ms/step - accuracy: 0.7142 - loss: 0.7005 - val_accuracy: 0.5700 - val_loss: 2.6377\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.7563 - loss: 0.6331 - val_accuracy: 0.6633 - val_loss: 1.9037\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7447 - loss: 0.6299 - val_accuracy: 0.6233 - val_loss: 2.3533\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7432 - loss: 0.6151 - val_accuracy: 0.6733 - val_loss: 1.6841\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7402 - loss: 0.6405 - val_accuracy: 0.7233 - val_loss: 1.2572\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7463 - loss: 0.6474 - val_accuracy: 0.7133 - val_loss: 1.3596\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 221ms/step - accuracy: 0.7416 - loss: 0.6202 - val_accuracy: 0.6167 - val_loss: 2.0940\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 213ms/step - accuracy: 0.7529 - loss: 0.5928 - val_accuracy: 0.7800 - val_loss: 0.8695\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 208ms/step - accuracy: 0.7808 - loss: 0.5700 - val_accuracy: 0.7900 - val_loss: 0.7762\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.7853 - loss: 0.5318 - val_accuracy: 0.8200 - val_loss: 0.6153\n",
      "Validation accuracy: 0.8199999928474426\n",
      "Training model with dropout=0.3 and learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 235ms/step - accuracy: 0.3879 - loss: 2.8559 - val_accuracy: 0.3400 - val_loss: 1.1144\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 219ms/step - accuracy: 0.4894 - loss: 1.8660 - val_accuracy: 0.3333 - val_loss: 1.1512\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 206ms/step - accuracy: 0.5223 - loss: 1.4097 - val_accuracy: 0.5133 - val_loss: 1.0873\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.5272 - loss: 1.2656 - val_accuracy: 0.4967 - val_loss: 1.1540\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.5358 - loss: 1.0891 - val_accuracy: 0.5933 - val_loss: 0.9509\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 0.5532 - loss: 1.0761 - val_accuracy: 0.6200 - val_loss: 0.8484\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.5477 - loss: 1.0161 - val_accuracy: 0.5567 - val_loss: 0.9545\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 174ms/step - accuracy: 0.5402 - loss: 1.0158 - val_accuracy: 0.6067 - val_loss: 0.9312\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.5920 - loss: 0.9500 - val_accuracy: 0.6867 - val_loss: 0.8560\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.5765 - loss: 0.9418 - val_accuracy: 0.6567 - val_loss: 0.9847\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.6072 - loss: 0.9334 - val_accuracy: 0.6900 - val_loss: 0.9271\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.6105 - loss: 0.8687 - val_accuracy: 0.7000 - val_loss: 0.9201\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.5865 - loss: 0.9045 - val_accuracy: 0.7033 - val_loss: 0.9311\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.6214 - loss: 0.8811 - val_accuracy: 0.6767 - val_loss: 1.0520\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.6380 - loss: 0.8097 - val_accuracy: 0.6867 - val_loss: 0.9854\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.6766 - loss: 0.7948 - val_accuracy: 0.6967 - val_loss: 0.9108\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.6771 - loss: 0.7421 - val_accuracy: 0.6667 - val_loss: 0.9480\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.6461 - loss: 0.8166 - val_accuracy: 0.6633 - val_loss: 0.9076\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.6387 - loss: 0.8283 - val_accuracy: 0.7067 - val_loss: 0.7471\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.6888 - loss: 0.7399 - val_accuracy: 0.6967 - val_loss: 0.7348\n",
      "Validation accuracy: 0.7066666483879089\n",
      "Training model with dropout=0.4 and learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 194ms/step - accuracy: 0.4177 - loss: 3.6698 - val_accuracy: 0.3333 - val_loss: 1.6527\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 0.5469 - loss: 1.0588 - val_accuracy: 0.3333 - val_loss: 2.7687\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.5216 - loss: 1.0179 - val_accuracy: 0.3333 - val_loss: 3.5570\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.6183 - loss: 0.8549 - val_accuracy: 0.3333 - val_loss: 4.3379\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 201ms/step - accuracy: 0.6272 - loss: 0.8057 - val_accuracy: 0.3333 - val_loss: 4.7969\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.6276 - loss: 0.8890 - val_accuracy: 0.3333 - val_loss: 6.4924\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.6550 - loss: 0.8112 - val_accuracy: 0.3333 - val_loss: 4.8170\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.6866 - loss: 0.7723 - val_accuracy: 0.3333 - val_loss: 5.6611\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.6620 - loss: 0.8103 - val_accuracy: 0.3600 - val_loss: 4.1113\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 0.6681 - loss: 0.8150 - val_accuracy: 0.4733 - val_loss: 3.0903\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7310 - loss: 0.7333 - val_accuracy: 0.5167 - val_loss: 3.7583\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.7190 - loss: 0.7871 - val_accuracy: 0.5333 - val_loss: 2.4994\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 211ms/step - accuracy: 0.7259 - loss: 0.6972 - val_accuracy: 0.6833 - val_loss: 1.5748\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7155 - loss: 0.6933 - val_accuracy: 0.6533 - val_loss: 1.4638\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 277ms/step - accuracy: 0.6989 - loss: 0.7166 - val_accuracy: 0.6867 - val_loss: 1.6318\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 310ms/step - accuracy: 0.7044 - loss: 0.7095 - val_accuracy: 0.6467 - val_loss: 1.9003\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 313ms/step - accuracy: 0.7327 - loss: 0.6645 - val_accuracy: 0.6900 - val_loss: 1.6040\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 309ms/step - accuracy: 0.7620 - loss: 0.6460 - val_accuracy: 0.7400 - val_loss: 0.9330\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 286ms/step - accuracy: 0.7273 - loss: 0.6701 - val_accuracy: 0.5800 - val_loss: 1.6488\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 289ms/step - accuracy: 0.7461 - loss: 0.6533 - val_accuracy: 0.7300 - val_loss: 0.8356\n",
      "Validation accuracy: 0.7400000095367432\n",
      "Training model with dropout=0.4 and learning_rate=0.0005\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 222ms/step - accuracy: 0.4079 - loss: 3.2043 - val_accuracy: 0.3333 - val_loss: 1.2054\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 239ms/step - accuracy: 0.4972 - loss: 1.3064 - val_accuracy: 0.3333 - val_loss: 2.7267\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - accuracy: 0.5425 - loss: 0.9917 - val_accuracy: 0.3333 - val_loss: 3.8386\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 202ms/step - accuracy: 0.5893 - loss: 0.9231 - val_accuracy: 0.3333 - val_loss: 3.7518\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6207 - loss: 0.8917 - val_accuracy: 0.3333 - val_loss: 2.9733\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 200ms/step - accuracy: 0.6344 - loss: 0.8491 - val_accuracy: 0.3767 - val_loss: 2.2628\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.6798 - loss: 0.8288 - val_accuracy: 0.4600 - val_loss: 1.4988\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 202ms/step - accuracy: 0.6557 - loss: 0.8002 - val_accuracy: 0.4833 - val_loss: 1.6487\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.6560 - loss: 0.8225 - val_accuracy: 0.5233 - val_loss: 1.3817\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.6892 - loss: 0.7883 - val_accuracy: 0.6467 - val_loss: 1.3626\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.6862 - loss: 0.7471 - val_accuracy: 0.6700 - val_loss: 1.5884\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.7016 - loss: 0.7193 - val_accuracy: 0.6833 - val_loss: 1.6521\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.6625 - loss: 0.7954 - val_accuracy: 0.7167 - val_loss: 1.3912\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.7082 - loss: 0.6719 - val_accuracy: 0.7200 - val_loss: 1.5358\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.6992 - loss: 0.7219 - val_accuracy: 0.6933 - val_loss: 1.2060\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.6838 - loss: 0.7743 - val_accuracy: 0.6833 - val_loss: 1.0708\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 211ms/step - accuracy: 0.7294 - loss: 0.7073 - val_accuracy: 0.7767 - val_loss: 0.8155\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 204ms/step - accuracy: 0.7169 - loss: 0.7100 - val_accuracy: 0.7067 - val_loss: 1.1229\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.7341 - loss: 0.6792 - val_accuracy: 0.7167 - val_loss: 1.6815\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.7225 - loss: 0.6834 - val_accuracy: 0.7000 - val_loss: 1.2668\n",
      "Validation accuracy: 0.7766666412353516\n",
      "Training model with dropout=0.4 and learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.3310 - loss: 4.2973 - val_accuracy: 0.3333 - val_loss: 1.4272\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.4229 - loss: 2.5182 - val_accuracy: 0.3333 - val_loss: 2.5413\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.4687 - loss: 1.8980 - val_accuracy: 0.3333 - val_loss: 3.0861\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.4941 - loss: 1.4347 - val_accuracy: 0.3333 - val_loss: 3.7666\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.4833 - loss: 1.3587 - val_accuracy: 0.3333 - val_loss: 3.8819\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.5129 - loss: 1.2254 - val_accuracy: 0.3333 - val_loss: 3.7652\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.4904 - loss: 1.2109 - val_accuracy: 0.3333 - val_loss: 3.6395\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 207ms/step - accuracy: 0.4999 - loss: 1.0985 - val_accuracy: 0.3333 - val_loss: 3.1671\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.5469 - loss: 1.0577 - val_accuracy: 0.3467 - val_loss: 2.8739\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 205ms/step - accuracy: 0.5521 - loss: 1.0234 - val_accuracy: 0.4667 - val_loss: 2.6216\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 248ms/step - accuracy: 0.5184 - loss: 1.0593 - val_accuracy: 0.5267 - val_loss: 2.3346\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 180ms/step - accuracy: 0.5497 - loss: 1.0538 - val_accuracy: 0.6200 - val_loss: 1.8740\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.5343 - loss: 0.9930 - val_accuracy: 0.6433 - val_loss: 1.7531\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - accuracy: 0.5598 - loss: 0.9925 - val_accuracy: 0.6567 - val_loss: 1.8230\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 196ms/step - accuracy: 0.5799 - loss: 0.9113 - val_accuracy: 0.6800 - val_loss: 1.7179\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 213ms/step - accuracy: 0.6013 - loss: 0.8798 - val_accuracy: 0.7067 - val_loss: 1.5412\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.6065 - loss: 0.9064 - val_accuracy: 0.7067 - val_loss: 1.5930\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - accuracy: 0.6337 - loss: 0.8661 - val_accuracy: 0.6933 - val_loss: 1.3617\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 255ms/step - accuracy: 0.5972 - loss: 0.9199 - val_accuracy: 0.6967 - val_loss: 1.2456\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 253ms/step - accuracy: 0.6030 - loss: 0.8867 - val_accuracy: 0.7000 - val_loss: 1.2059\n",
      "Validation accuracy: 0.7066666483879089\n",
      "Training model with dropout=0.5 and learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 221ms/step - accuracy: 0.4203 - loss: 4.3295 - val_accuracy: 0.3333 - val_loss: 2.0833\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 211ms/step - accuracy: 0.4452 - loss: 1.3332 - val_accuracy: 0.3333 - val_loss: 3.0092\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.4403 - loss: 1.0991 - val_accuracy: 0.3333 - val_loss: 4.4483\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 180ms/step - accuracy: 0.5176 - loss: 1.0297 - val_accuracy: 0.3333 - val_loss: 4.5929\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 0.5173 - loss: 1.0361 - val_accuracy: 0.3467 - val_loss: 4.4258\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - accuracy: 0.5165 - loss: 1.0084 - val_accuracy: 0.3333 - val_loss: 7.0475\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.5215 - loss: 0.9754 - val_accuracy: 0.3433 - val_loss: 5.7747\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.6091 - loss: 0.9098 - val_accuracy: 0.3633 - val_loss: 5.1069\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 203ms/step - accuracy: 0.5817 - loss: 0.9375 - val_accuracy: 0.4100 - val_loss: 5.4281\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 174ms/step - accuracy: 0.6013 - loss: 0.9458 - val_accuracy: 0.4067 - val_loss: 5.7271\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.5789 - loss: 0.8767 - val_accuracy: 0.4200 - val_loss: 5.2758\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 180ms/step - accuracy: 0.5683 - loss: 0.8636 - val_accuracy: 0.4433 - val_loss: 4.0501\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.6033 - loss: 0.8401 - val_accuracy: 0.5067 - val_loss: 3.6882\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.6229 - loss: 0.8607 - val_accuracy: 0.5600 - val_loss: 2.9726\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 0.6298 - loss: 0.8128 - val_accuracy: 0.5767 - val_loss: 2.8973\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.6285 - loss: 0.8541 - val_accuracy: 0.6200 - val_loss: 2.4199\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 212ms/step - accuracy: 0.6571 - loss: 0.8038 - val_accuracy: 0.6500 - val_loss: 2.7035\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.6276 - loss: 0.8183 - val_accuracy: 0.6400 - val_loss: 1.9440\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 199ms/step - accuracy: 0.6438 - loss: 0.8315 - val_accuracy: 0.6267 - val_loss: 2.1263\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - accuracy: 0.6512 - loss: 0.8253 - val_accuracy: 0.6067 - val_loss: 1.9969\n",
      "Validation accuracy: 0.6499999761581421\n",
      "Training model with dropout=0.5 and learning_rate=0.0005\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 196ms/step - accuracy: 0.3803 - loss: 4.6263 - val_accuracy: 0.3333 - val_loss: 2.1746\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.4402 - loss: 1.7084 - val_accuracy: 0.3333 - val_loss: 4.2539\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 136ms/step - accuracy: 0.4326 - loss: 1.2406 - val_accuracy: 0.3333 - val_loss: 4.9894\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.4753 - loss: 1.1235 - val_accuracy: 0.3333 - val_loss: 4.1340\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.5100 - loss: 1.0387 - val_accuracy: 0.3333 - val_loss: 4.0574\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.4988 - loss: 1.0148 - val_accuracy: 0.3333 - val_loss: 3.9052\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.5420 - loss: 0.9919 - val_accuracy: 0.3333 - val_loss: 4.2254\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.5531 - loss: 0.9605 - val_accuracy: 0.3433 - val_loss: 4.0202\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.5436 - loss: 0.9929 - val_accuracy: 0.3667 - val_loss: 3.8811\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.5734 - loss: 0.9594 - val_accuracy: 0.4600 - val_loss: 3.4013\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.5868 - loss: 0.9174 - val_accuracy: 0.5067 - val_loss: 3.2177\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.5851 - loss: 0.9091 - val_accuracy: 0.6133 - val_loss: 2.5400\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - accuracy: 0.5917 - loss: 0.8647 - val_accuracy: 0.6167 - val_loss: 2.5227\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.6123 - loss: 0.8610 - val_accuracy: 0.6333 - val_loss: 2.5784\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.6143 - loss: 0.8989 - val_accuracy: 0.6767 - val_loss: 2.0438\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.6166 - loss: 0.8774 - val_accuracy: 0.6767 - val_loss: 2.1302\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - accuracy: 0.6279 - loss: 0.8583 - val_accuracy: 0.6567 - val_loss: 2.3651\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.6456 - loss: 0.8153 - val_accuracy: 0.7033 - val_loss: 1.8793\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.6396 - loss: 0.8272 - val_accuracy: 0.7167 - val_loss: 1.7361\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.6432 - loss: 0.8584 - val_accuracy: 0.7133 - val_loss: 1.6216\n",
      "Validation accuracy: 0.7166666388511658\n",
      "Training model with dropout=0.5 and learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.3110 - loss: 5.6543 - val_accuracy: 0.3333 - val_loss: 1.1407\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.3813 - loss: 3.5767 - val_accuracy: 0.3333 - val_loss: 1.3708\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.4053 - loss: 2.8204 - val_accuracy: 0.3333 - val_loss: 1.5994\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.4362 - loss: 2.2083 - val_accuracy: 0.3333 - val_loss: 1.7906\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.3803 - loss: 1.8721 - val_accuracy: 0.3333 - val_loss: 1.9931\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - accuracy: 0.4212 - loss: 1.5973 - val_accuracy: 0.3367 - val_loss: 1.9818\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.4060 - loss: 1.5120 - val_accuracy: 0.3600 - val_loss: 1.9013\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - accuracy: 0.4336 - loss: 1.4098 - val_accuracy: 0.3667 - val_loss: 1.8847\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.3829 - loss: 1.2828 - val_accuracy: 0.3900 - val_loss: 1.8429\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.4011 - loss: 1.2850 - val_accuracy: 0.4700 - val_loss: 1.6993\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - accuracy: 0.4415 - loss: 1.1962 - val_accuracy: 0.5400 - val_loss: 1.4702\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.4497 - loss: 1.1762 - val_accuracy: 0.6133 - val_loss: 1.3276\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.4078 - loss: 1.2026 - val_accuracy: 0.6433 - val_loss: 1.3299\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.4115 - loss: 1.1383 - val_accuracy: 0.6333 - val_loss: 1.4818\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.4410 - loss: 1.1193 - val_accuracy: 0.6633 - val_loss: 1.4002\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.4384 - loss: 1.1678 - val_accuracy: 0.6733 - val_loss: 1.3996\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.4624 - loss: 1.1114 - val_accuracy: 0.6633 - val_loss: 1.4353\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.4689 - loss: 1.0607 - val_accuracy: 0.6767 - val_loss: 1.4367\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - accuracy: 0.4697 - loss: 1.0839 - val_accuracy: 0.6800 - val_loss: 1.5155\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.4883 - loss: 1.0523 - val_accuracy: 0.6833 - val_loss: 1.5318\n",
      "Validation accuracy: 0.6833333373069763\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5273 - loss: 2.2736\n",
      "Best Hyperparameter Tuned Model - Test Accuracy: 0.6066666841506958\n",
      "Best Hyperparameter Tuned Model - Test Loss: 2.20161771774292\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/stepWARNING:tensorflow:5 out of the last 31 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3097feb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 31 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3097feb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Best Hyperparameter Tuned Model - Confusion Matrix:\n",
      "[[26  0 24]\n",
      " [ 7 16 27]\n",
      " [ 0  1 49]]\n",
      "Best Hyperparameter Tuned Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Food Waste       0.79      0.52      0.63        50\n",
      "Mixed Recycle       0.94      0.32      0.48        50\n",
      "        Trash       0.49      0.98      0.65        50\n",
      "\n",
      "     accuracy                           0.61       150\n",
      "    macro avg       0.74      0.61      0.59       150\n",
      " weighted avg       0.74      0.61      0.59       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------- EXPERIMENT 2: Hyperparameter Tuning --------------------\n",
    "print(\"Experiment 2: Hyperparameter Tuning\")\n",
    "dropout_rates = [0.3, 0.4, 0.5]\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "best_model = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "def build_model(dropout_rate=0.5, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(*image_size, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate + 0.1),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate + 0.2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate + 0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Training model with dropout={dropout_rate} and learning_rate={lr}\")\n",
    "        model = build_model(dropout_rate=dropout_rate, learning_rate=lr)\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "        plot_history(history, f\"Dropout={dropout_rate}, LR={lr}\")\n",
    "        val_accuracy = max(history.history['val_accuracy'])\n",
    "        print(f\"Validation accuracy: {val_accuracy}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "            model.save('best_model.h5')\n",
    "\n",
    "evaluate_model(best_model, \"Best Hyperparameter Tuned Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3: Data Augmentation\n",
      "Found 1050 images belonging to 3 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.3904 - loss: 3.7100 - val_accuracy: 0.4900 - val_loss: 2.2413\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.4738 - loss: 1.3599 - val_accuracy: 0.4700 - val_loss: 3.4348\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - accuracy: 0.4959 - loss: 1.1181 - val_accuracy: 0.3367 - val_loss: 4.0437\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.4578 - loss: 1.1029 - val_accuracy: 0.3433 - val_loss: 5.7203\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.5081 - loss: 1.0514 - val_accuracy: 0.3467 - val_loss: 7.0814\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - accuracy: 0.5686 - loss: 0.9853 - val_accuracy: 0.3467 - val_loss: 6.8161\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.5844 - loss: 0.9541 - val_accuracy: 0.4267 - val_loss: 5.8141\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - accuracy: 0.5998 - loss: 0.9395 - val_accuracy: 0.4667 - val_loss: 5.4396\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.5814 - loss: 0.9323 - val_accuracy: 0.4800 - val_loss: 5.4060\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.5918 - loss: 0.9311 - val_accuracy: 0.4667 - val_loss: 5.3331\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.5888 - loss: 0.8609 - val_accuracy: 0.4800 - val_loss: 4.6571\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.6212 - loss: 0.8882 - val_accuracy: 0.5067 - val_loss: 4.5431\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.6223 - loss: 0.8480 - val_accuracy: 0.5500 - val_loss: 4.1431\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.6226 - loss: 0.8348 - val_accuracy: 0.6300 - val_loss: 2.7338\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - accuracy: 0.6787 - loss: 0.7565 - val_accuracy: 0.6667 - val_loss: 2.0655\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.6224 - loss: 0.8355 - val_accuracy: 0.6833 - val_loss: 1.9238\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.6939 - loss: 0.7406 - val_accuracy: 0.7067 - val_loss: 1.7595\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.6773 - loss: 0.7970 - val_accuracy: 0.7067 - val_loss: 1.9288\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.6504 - loss: 0.7633 - val_accuracy: 0.6233 - val_loss: 1.7956\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.6485 - loss: 0.8456 - val_accuracy: 0.5733 - val_loss: 2.3941\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7674 - loss: 1.1741\n",
      "Augmented Data Model - Test Accuracy: 0.6666666865348816\n",
      "Augmented Data Model - Test Loss: 1.7322065830230713\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Augmented Data Model - Confusion Matrix:\n",
      "[[48  1  1]\n",
      " [22 26  2]\n",
      " [21  3 26]]\n",
      "Augmented Data Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Food Waste       0.53      0.96      0.68        50\n",
      "Mixed Recycle       0.87      0.52      0.65        50\n",
      "        Trash       0.90      0.52      0.66        50\n",
      "\n",
      "     accuracy                           0.67       150\n",
      "    macro avg       0.76      0.67      0.66       150\n",
      " weighted avg       0.76      0.67      0.66       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------- EXPERIMENT 3: Data Augmentation --------------------\n",
    "print(\"Experiment 3: Data Augmentation\")\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,       # Slightly reduce rotations\n",
    "    width_shift_range=0.1,   # Limit shifts\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2]  # Realistic lighting variations\n",
    ")\n",
    "\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "model_augmented = build_model(dropout_rate=0.4, learning_rate=0.0005)\n",
    "history_augmented = model_augmented.fit(\n",
    "    train_generator_augmented,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "plot_history(history_augmented, \"Augmented Data Model\")\n",
    "evaluate_model(model_augmented, \"Augmented Data Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4: Adding Extra Convolutional Layers\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 129ms/step - accuracy: 0.4222 - loss: 2.3887 - val_accuracy: 0.3333 - val_loss: 1.3755\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.5464 - loss: 1.3373 - val_accuracy: 0.3333 - val_loss: 2.0680\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.5773 - loss: 1.0849 - val_accuracy: 0.3333 - val_loss: 3.6170\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.6408 - loss: 0.8850 - val_accuracy: 0.3333 - val_loss: 2.8182\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.6723 - loss: 0.7914 - val_accuracy: 0.3333 - val_loss: 3.6434\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - accuracy: 0.6970 - loss: 0.7752 - val_accuracy: 0.3333 - val_loss: 4.1966\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.6597 - loss: 0.8185 - val_accuracy: 0.3467 - val_loss: 2.9143\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.7135 - loss: 0.7305 - val_accuracy: 0.3367 - val_loss: 2.9976\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.6976 - loss: 0.7160 - val_accuracy: 0.3433 - val_loss: 2.5967\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.6968 - loss: 0.6873 - val_accuracy: 0.4433 - val_loss: 1.9750\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.7096 - loss: 0.6858 - val_accuracy: 0.4200 - val_loss: 2.2518\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - accuracy: 0.7531 - loss: 0.6190 - val_accuracy: 0.6400 - val_loss: 1.2903\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.7513 - loss: 0.6150 - val_accuracy: 0.5367 - val_loss: 1.7978\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.7459 - loss: 0.6221 - val_accuracy: 0.5633 - val_loss: 1.7468\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.7357 - loss: 0.6320 - val_accuracy: 0.5900 - val_loss: 1.6521\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.7631 - loss: 0.5744 - val_accuracy: 0.6600 - val_loss: 1.1417\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.7838 - loss: 0.5508 - val_accuracy: 0.7533 - val_loss: 0.8970\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.7569 - loss: 0.5898 - val_accuracy: 0.7700 - val_loss: 0.7891\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - accuracy: 0.7678 - loss: 0.5692 - val_accuracy: 0.6033 - val_loss: 1.7209\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.7877 - loss: 0.5767 - val_accuracy: 0.6367 - val_loss: 1.4982\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7124 - loss: 1.1347\n",
      "Model with Extra Layers - Test Accuracy: 0.6866666674613953\n",
      "Model with Extra Layers - Test Loss: 1.215235710144043\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Model with Extra Layers - Confusion Matrix:\n",
      "[[46  0  4]\n",
      " [12 10 28]\n",
      " [ 3  0 47]]\n",
      "Model with Extra Layers - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Food Waste       0.75      0.92      0.83        50\n",
      "Mixed Recycle       1.00      0.20      0.33        50\n",
      "        Trash       0.59      0.94      0.73        50\n",
      "\n",
      "     accuracy                           0.69       150\n",
      "    macro avg       0.78      0.69      0.63       150\n",
      " weighted avg       0.78      0.69      0.63       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------- EXPERIMENT 4: Adding Extra Layers --------------------\n",
    "print(\"Experiment 4: Adding Extra Convolutional Layers\")\n",
    "\n",
    "def build_model_with_extra_layers(dropout_rate=0.4, learning_rate=0.0005):\n",
    "    model = Sequential([\n",
    "        # First set of convolutional layers\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(*image_size, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Adding additional convolutional layer\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu'),  # Extra layer\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),  # Increased units\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_extra_layers = build_model_with_extra_layers()\n",
    "history_extra_layers = model_extra_layers.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "plot_history(history_extra_layers, \"Model with Extra Layers\")\n",
    "evaluate_model(model_extra_layers, \"Model with Extra Layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 5: Adding Extra Dense Layers\n",
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - accuracy: 0.3830 - loss: 2.4952 - val_accuracy: 0.3600 - val_loss: 1.4783\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.5476 - loss: 1.2795 - val_accuracy: 0.3333 - val_loss: 1.8986\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.5855 - loss: 1.0740 - val_accuracy: 0.3333 - val_loss: 2.3846\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.5791 - loss: 0.9672 - val_accuracy: 0.3333 - val_loss: 2.5370\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - accuracy: 0.6397 - loss: 0.8986 - val_accuracy: 0.3333 - val_loss: 2.2840\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.6503 - loss: 0.8134 - val_accuracy: 0.3333 - val_loss: 3.2928\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.6580 - loss: 0.8312 - val_accuracy: 0.3333 - val_loss: 3.6218\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.6919 - loss: 0.7941 - val_accuracy: 0.3333 - val_loss: 3.3996\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.7113 - loss: 0.7539 - val_accuracy: 0.3333 - val_loss: 3.1603\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.7204 - loss: 0.7004 - val_accuracy: 0.3367 - val_loss: 3.0741\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.7064 - loss: 0.7425 - val_accuracy: 0.3367 - val_loss: 2.8450\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.7027 - loss: 0.7082 - val_accuracy: 0.3467 - val_loss: 2.3508\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.7507 - loss: 0.6490 - val_accuracy: 0.4233 - val_loss: 1.4815\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.7316 - loss: 0.6607 - val_accuracy: 0.5067 - val_loss: 1.1018\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.7538 - loss: 0.6465 - val_accuracy: 0.5067 - val_loss: 1.1788\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.7307 - loss: 0.6513 - val_accuracy: 0.5267 - val_loss: 1.0846\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.7698 - loss: 0.6298 - val_accuracy: 0.5833 - val_loss: 0.9893\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.7431 - loss: 0.6245 - val_accuracy: 0.6567 - val_loss: 0.9040\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.7350 - loss: 0.6691 - val_accuracy: 0.6733 - val_loss: 0.8463\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - accuracy: 0.7336 - loss: 0.6668 - val_accuracy: 0.7233 - val_loss: 0.8109\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6899 - loss: 0.9382\n",
      "Model with Extra Dense Layers - Test Accuracy: 0.753333330154419\n",
      "Model with Extra Dense Layers - Test Loss: 0.6789380311965942\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Model with Extra Dense Layers - Confusion Matrix:\n",
      "[[26 17  7]\n",
      " [ 0 46  4]\n",
      " [ 3  6 41]]\n",
      "Model with Extra Dense Layers - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Food Waste       0.90      0.52      0.66        50\n",
      "Mixed Recycle       0.67      0.92      0.77        50\n",
      "        Trash       0.79      0.82      0.80        50\n",
      "\n",
      "     accuracy                           0.75       150\n",
      "    macro avg       0.78      0.75      0.75       150\n",
      " weighted avg       0.78      0.75      0.75       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------- EXPERIMENT 5: Increasing Dense Layers --------------------\n",
    "print(\"Experiment 5: Adding Extra Dense Layers\")\n",
    "\n",
    "def build_model_with_dense_layers(dropout_rate=0.4, learning_rate=0.0005):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(*image_size, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),  # Increased units\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),  # Extra dense layer\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_dense_layers = build_model_with_dense_layers()\n",
    "history_dense_layers = model_dense_layers.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "plot_history(history_dense_layers, \"Model with Extra Dense Layers\")\n",
    "evaluate_model(model_dense_layers, \"Model with Extra Dense Layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 212ms/step - accuracy: 0.5557 - loss: 2.3979 - val_accuracy: 0.3767 - val_loss: 1.0837 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6346 - loss: 1.7415 - val_accuracy: 0.3333 - val_loss: 3.4004 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.6486 - loss: 1.3600 - val_accuracy: 0.3333 - val_loss: 2.4759 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.6826 - loss: 1.0881\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.6826 - loss: 1.0863 - val_accuracy: 0.4100 - val_loss: 1.9132 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - accuracy: 0.7082 - loss: 0.8864 - val_accuracy: 0.3933 - val_loss: 1.7409 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 214ms/step - accuracy: 0.7528 - loss: 0.6199 - val_accuracy: 0.3600 - val_loss: 1.8727 - learning_rate: 5.0000e-04\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.3881 - loss: 1.0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3766666650772095\n",
      "Validation Loss: 1.0836825370788574\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "train_dir = 'waste-images/train'\n",
    "val_dir = 'waste-images/val'\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Build the Model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layers with Batch Normalization\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten and Fully Connected Layers with Dropout\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))  # Adjust for the number of classes\n",
    "\n",
    "# Compile the Model\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate on Validation Data\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "# Save the Model\n",
    "model.save('improved_model.h5')\n",
    "\n",
    "# Visualize Training and Validation Accuracy/Loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model architecture\n",
    "def build_waste_classification_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Convolutional Layers\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Data augmentation\n",
    "def create_data_generators(train_dir, val_dir, image_size, batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Define parameters\n",
    "input_shape = (64, 64, 3)  # Image size (height, width, channels)\n",
    "num_classes = 3  # Food Waste, Mixed Recycle, Trash\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "train_dir = 'waste-images/train'\n",
    "val_dir = 'waste-images/val'\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = build_waste_classification_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate data\n",
    "train_generator, val_generator = create_data_generators(train_dir, val_dir, (64, 64), batch_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('waste_classification_model.h5')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model architecture\n",
    "def build_waste_classification_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Convolutional Layers\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Data augmentation\n",
    "def create_data_generators(train_dir, val_dir, image_size, batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Define parameters\n",
    "input_shape = (64, 64, 3)  # Image size (height, width, channels)\n",
    "num_classes = 3  # Food Waste, Mixed Recycle, Trash\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "train_dir = 'waste-images/train'\n",
    "val_dir = 'waste-images/val'\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = build_waste_classification_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate data\n",
    "train_generator, val_generator = create_data_generators(train_dir, val_dir, (64, 64), batch_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('waste_classification_model.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
